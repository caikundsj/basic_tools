job {
  # job任务名称前缀
  name.prefix = "flink_"
  # 流计算程序默认处理并行度
  default.parallelism = 3
  # Sink算子单独写并行度,防止Flink内部数据积压
  sink.parallelism = 1
  checkpoint {
    # 是否开启checkpoint
    enable = true
    # checkpoint时间间隔 30S
    interval = 30000
    # 指定checkpoint类型,本地测试可以设置为HASHMAP_STATE(HASHMAP_STATE/EMBEDDED_ROCKS_STATE)
    type = "MEMORY_STATE"
    # 指定为hdfs时候需要指定checkpoint路径 (hdfs://localhost:9000/user/flink/checkpoints)
    path = "file:///checkpoints"
    # checkpoint超时时间
    timeout = 60000
    # 两次CheckPoint中间最小时间间隔
    min.pause.between = 5000
    # 同时允许多少个Checkpoint在做快照
    current.checkpoints = 1
    # job遇到问题时,默认使用FixedDelayRestartStrategy重启策略的重试次数(3次)
    restart.attempts.times = 3
    # job遇到问题时,默认使用FixedDelayRestartStrategy重启策略的的每次重启时间间隔(30S)
    restart.attempts.interval = 30000
  }
  savepoint {
    enable = false
    path = ""
  }
  table {
   # planner选择 默认不选(Blink成为唯一实现)
   planner = ""
   # 设置Planner方式 BATCH/STREAMING
   runtime = BATCH
  }
}

kafka {
  # 集群地址
  bootstrap.servers = "10.10.5.150:9092,10.10.5.151:9092,10.10.5.152:9092,"
  # 每个Batch要存放size字节数据后才可以发送出去
  batch.size = "131072"
  # Batch创建之后,过多久发送出去
  linger.ms = "100"
  # 缓存大小
  buffer.memory = "67108864"
  # key 序列化
  key.serializer = "org.apache.kafka.common.serialization.StringSerializer"
  # value 序列化
  value.serializer = "org.apache.kafka.common.serialization.StringSerializer"
  group.id = "fk_sdc_stat_collect_measuring"
  # 间隔多久（interval）获取一次kakfa的元数据,防止Kafka的topic进行扩容，出现丢数据的情况。大于0开启
  partition.discover.millis = 30000
  topic {
      MeasuringTopic: "sdc_stat_collect_measuring"
  }
}

mysql {
  url: "jdbc:mysql://10.10.62.21:3306/ess_equipment"
  driver: "com.mysql.cj.jdbc.Driver"
  username: "root"
  password: "admin@hckj"
}
hiveCatalog {
  catalogName: "hive_catalog"
  dataBase: "dwd"
  hiveConf: "D:/hiveConf"
  hiveVersion: "2.1.1"
}
clickhouse {
  url: "clickhouse://10.10.5.150:8126"
  database-name: "ess_cloud"
  username: "default"
  password: "123456"
}

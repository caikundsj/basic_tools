job {
  # job任务名称前缀
  name.prefix = "flink_"
  # 流计算程序默认处理并行度
  default.parallelism = 3
  # Sink算子单独写并行度,防止Flink内部数据积压
  sink.parallelism = 1
  checkpoint {
    # 是否开启checkpoint
    enable = true
    # checkpoint时间间隔 30S
    interval = 30000
    # 指定checkpoint类型,本地测试可以设置为Memory(memory/hdfs) MEMORY_STATE| HDFS_STATE
    type = "HDFS_STATE"
    # 指定为hdfs时候需要指定checkpoint路径 (hdfs://localhost:9000/user/flink/checkpoints)
    path = "hdfs://hacluster/flink/checkpoints"
    # checkpoint超时时间
    timeout = 60000
    # 两次CheckPoint中间最小时间间隔
    min.pause.between = 5000
    # 同时允许多少个Checkpoint在做快照
    current.checkpoints = 1
    # job遇到问题时,默认使用FixedDelayRestartStrategy重启策略的重试次数(3次)
    restart.attempts.times = 3
    # job遇到问题时,默认使用FixedDelayRestartStrategy重启策略的的每次重启时间间隔(30S)
    restart.attempts.interval = 30000
  }
}
kafka {
  # 集群地址
  bootstrap.servers = "192.168.64.227:9092,192.168.64.228:9092,192.168.64.184:9092,192.168.64.196:9092,192.168.64.162:9092"
  # 每个Batch要存放size字节数据后才可以发送出去
  batch.size = "131072"
  # Batch创建之后,过多久发送出去
  linger.ms = "100"
  # 缓存大小
  buffer.memory = "67108864"
  # key 序列化
  key.serializer = "org.apache.kafka.common.serialization.StringSerializer"
  # value 序列化
  value.serializer = "org.apache.kafka.common.serialization.StringSerializer"
  group.id = "fk_sdc_stat_collect_measuring"
  # 间隔多久（interval）获取一次kakfa的元数据,防止Kafka的topic进行扩容，出现丢数据的情况。大于0开启
  partition.discover.millis = 30000
  topic {
      MeasuringTopic: "sdc_stat_collect_measuring"
  }
}
mysql {
  url: "jdbc:mysql://192.168.64.174:3306/ess_equipment"
  driver: "com.mysql.cj.jdbc.Driver"
  username: "root"
  password: "Miaosh@2022"
}
hiveCatalog {
  catalogName: "hive_catalog"
  dataBase: "dwd"
  hiveConf: "/opt/Bigdata/client/Flink/flink/conf"
  hiveVersion: "3.1.0"
}
clickhouse {
  url: "clickhouse://192.168.64.16:8123"
  database-name: "ess_cloud"
  username: "clickhouse"
  password: "eW3}cU5-lX"
}
